# -*- coding: utf-8 -*-
"""CompFor_hw4_writeup_yiruxion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUgV7F-qsWDhsz06bHFwDx3U7xue7HMO

## Deep Learning Classifier for Detecting DeepFake Forgeries in Videos

The model implemented was adapted from MesoNet-4 architecture from this [paper](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1809.00888.pdf) by Afchar et al.

#### Overview of MesoNet-4

MesoNet-4 is composed of 4 CNN layers (with MaxPooling after each layer) followed by a fully connected layer with one hidden layer which outputs the probability of an image being real or fake.  

#### Architecture

Below is a figure from the paper detailing the network architecture.

<center><img src="https://drive.google.com/uc?export=view&id=1My8UQ-l3FijeBRGTG_YhoaY4JnA3xpP8" width=400/></center>

#### Dataset
The dataset has been provided by the MesoNet authors. The colored face images are aligned, of size `256x256`. Training and testing datasets include both real and deepfaked images
"""

# Check if GPU exists - Tesla 4
!nvidia-smi

# set the seed to get reproducable results
from numpy.random import seed
seed(1)
#from tensorflow import set_random_seed
import tensorflow
tensorflow.random.set_seed(2)

import tensorflow as tf

import keras

from keras.models import Model

from tensorflow.keras.preprocessing.image import ImageDataGenerator
# get keras layers
from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU
# we will use the Adam optimizer for this assignment
from keras.optimizers import Adam

# define the image width
IMAGE_WIDTH = 256
# define the image height
IMAGE_HEIGHT = 256
# define the image channels
IMAGE_CHANNELS = 3
# define the learning rate
LEARNING_RATE = 0.0001
# batch size for the ImageDataGenerator
BATCHSIZE = 30

class MesoNet4:
    def __init__(self):
        self.model = self.get_model()

    def get_model(self):
        x = Input(shape = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))

        # first block
        layer1 = Conv2D(8, (3,3), padding='same', activation='relu')(x)
        layer1 = BatchNormalization()(layer1)
        layer1 = MaxPooling2D(pool_size=(2,2), padding='same')(layer1)

        # second block
        layer2 = Conv2D(8, (5,5), padding='same', activation='relu')(layer1)
        layer2 = BatchNormalization()(layer2)
        layer2 = MaxPooling2D(pool_size=(2,2), padding='same')(layer2)

        # thrid block
        layer3 = Conv2D(16, (5,5), padding='same', activation='relu')(layer2)
        layer3 = BatchNormalization()(layer3)
        layer3 = MaxPooling2D(pool_size=(2,2), padding='same')(layer3)

        # fourth block
        layer4 = Conv2D(16, (5,5), padding='same', activation='relu')(layer3)
        layer4 = BatchNormalization()(layer4)
        conv_output = MaxPooling2D(pool_size=(4,4), padding='same')(layer4)

        # conv_output is the output of the last maxpool
        # we need to fallen the output of the conv layers
        y = Flatten()(conv_output)
        # add dropout layer
        y = Dropout(0.5)(y)
        # ADD THE FULLY CONNECTED 16
        y = Dense(16)(y)
        # add another dropout layer
        y = Dropout(0.5)(y)

        # y is the output of Dropout
        # we add one dense layer with sigmoid
        y = Dense(1, activation = 'sigmoid')(y)
        # now we return the model
        return Model(inputs = x, outputs = y)

train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory("/content/drive/MyDrive/data/train/",
                                                    target_size = (256, 256),
                                                    batch_size = BATCHSIZE,
                                                    class_mode = 'binary')

test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory("/content/drive/MyDrive/data/test/",
                                                    target_size = (256, 256),
                                                    batch_size = BATCHSIZE,
                                                    class_mode = 'binary')

# check the data
train_generator.class_indices

test_generator.class_indices

## define an optimizer
opt = Adam(learning_rate=LEARNING_RATE)

## Compile the model
NETWORK = MesoNet4()
NETWORK.model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])

"""## Train the model"""

# use NETWORK.model to access the model
# loss: 0.0604 - accuracy: 0.9325
EPOCHS = 3
trainer = NETWORK.model.fit_generator(generator=train_generator,
                                      steps_per_epoch = len(train_generator),
                                      epochs = EPOCHS,
                                      verbose =1)

NETWORK.model.save("/content/drive/MyDrive/meso")
NETWORK.model.save_weights("/content/drive/MyDrive/meso_weights")

"""## Test the model"""

# use NETWORK.model to access the model
# loss and metrics(accuracy)
test_scores = NETWORK.model.evaluate_generator(generator=test_generator)
print("The accuracy of the trained model is: ", test_scores[1])

"""The training accuracy is 0.9325, and the accuracy of the trained model on test data is 0.8409090638160706"""